{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path, bool=True):\n",
    "    with open(path, encoding=\"utf-8\") as filepath:\n",
    "        data = []\n",
    "        line_lst = []\n",
    "        for line in filepath:\n",
    "            if line != \"\\n\":\n",
    "                if bool:\n",
    "                    tokens = line.strip().split()\n",
    "                    line_lst.append((\" \".join(tokens[: -1]), tokens[-1]))\n",
    "                else:\n",
    "                    line_lst.append((line.strip(), ))\n",
    "            else:\n",
    "                data.append(line_lst)\n",
    "                line_lst = []\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(path, data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as filepath:\n",
    "        for data_str in data:\n",
    "            for line in data_str:\n",
    "                filepath.write(f\"{line[0]} {line[1]}\\n\")\n",
    "            filepath.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Write a function that estimates the emission parameters from the training set using MLE (maximum\n",
    "likelihood estimation):\n",
    "\n",
    "`e(x|y) = Count(y → x) / Count(y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimate_with_mle(dataset):\n",
    "    words = set([x for line in dataset for (x, y) in line])\n",
    "    y_count = defaultdict(int)\n",
    "    xy_count = defaultdict(lambda  : y_count)\n",
    "\n",
    "    for line in dataset:\n",
    "        for word, label in line:\n",
    "            y_count[label] += 1\n",
    "            xy_count[label][word] += 1\n",
    "\n",
    "    def est(x, y):\n",
    "        return xy_count[y][x] / y_count[y]\n",
    "        \n",
    "    return est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with estimating the emission parameters is that some words that appear in the test set\n",
    "do not appear in the training set. One simple idea to handle this issue is as follows. We introduce\n",
    "a special word token `#UNK#`, and make the following modifications to the computation of emission\n",
    "probabilities:\n",
    "\n",
    "```\n",
    "e(x|y) = Count(y→x) / Count(y)+k, \n",
    "If the word token x appears in the training set\n",
    "\n",
    "e(x|y) = k / Count(y)+k,\n",
    "If word token x is the special token #UNK#\n",
    "```\n",
    "\n",
    "(This basically says we assume from any label y there is a certain chance of generating `#UNK#` as\n",
    "a rare event, and empirically we assume we have observed that there are k occurrences of such an\n",
    "event.)\n",
    "\n",
    "During the testing phase, if the word does not appear in the training set, we replace that word with\n",
    "`#UNK#`.\n",
    "\n",
    "Set k to 1, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimate(dataset, k=1):\n",
    "    words = set([x for line in dataset for (x, y) in line])\n",
    "    y_count = defaultdict(int)\n",
    "    xy_count = defaultdict(lambda  : y_count)\n",
    "    \n",
    "    for line in dataset:\n",
    "        for word, label in line:\n",
    "            y_count[label] += 1\n",
    "            xy_count[label][word] += 1\n",
    "            \n",
    "    def e_para(x, y):\n",
    "        if x in words:\n",
    "            return xy_count[y][x]/(y_count[y] + k)\n",
    "        else:\n",
    "            return k/(y_count[y] + k)\n",
    "\n",
    "    return e_para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the datasets `RU`, `ES`, learn these parameters with `train`, and evaluate your system on the\n",
    "development set `dev.in` for each of the dataset. Write your output to `dev.p1.out` for the two\n",
    "datasets respectively.\n",
    "\n",
    "Compare your outputs and the gold-standard outputs in `dev.out` and report\n",
    "the precision, recall and F scores of such a baseline system for each dataset.\n",
    "\n",
    "The precision score is defined as follows:\n",
    "\n",
    "`Precision = (Total number f correctly predicted entities) / (Total number of predicted entities)`\n",
    "\n",
    "The recall score is defined as follows:\n",
    "\n",
    "`Recall = (Total number of correctly predicted entities) / (Total number of gold entities)`\n",
    "\n",
    "where a gold entity is a true entity that is annotated in the reference output file, and a predicted entity\n",
    "is regarded as correct if and only if it matches exactly the gold entity (i.e., both their boundaries and\n",
    "sentiment are exactly the same).\n",
    "\n",
    "Finally the F score is defined as follows:\n",
    "\n",
    "`F = 2 / [(1/Precision) + (1/Recall)]`\n",
    "\n",
    "*Note: in some cases, you might have an output sequence that consists of a transition from `O` to\n",
    "`I-negative` (rather than `B-negative`). For example, “`O I-negative I-negative O`”.\n",
    "In this case, the second and third words should be regarded as one entity with negative sentiment.*\n",
    "\n",
    "You can use the evaluation script shared with you to calculate such scores. However it is strongly\n",
    "encouraged that you understand how the scores are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(dataset, trainset):\n",
    "    ys = set([y for line in trainset for (x,y) in line])\n",
    "    estimate = get_estimate(trainset)\n",
    "    func_output = []\n",
    "    for line in dataset:\n",
    "        labels = []\n",
    "        for word, in line:\n",
    "            max_p = 0\n",
    "            label = None\n",
    "            for y in ys:\n",
    "                p = estimate(word, y)\n",
    "                if p > max_p:\n",
    "                    max_p = p\n",
    "                    label = y\n",
    "            labels.append((word, label))\n",
    "        func_output.append(labels)\n",
    "    return func_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_trainSet = read_dataset(\"./ES/ES/train\")\n",
    "es_dev = read_dataset(\"./ES/ES/dev.in\", False)\n",
    "es_dev_label = labelling(es_dev, es_trainSet)\n",
    "write_dataset(\"./ES/ES/dev.p2.out\", es_dev_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*insert some report stuff*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_trainSet = read_dataset(\"./RU/RU/train\")\n",
    "ru_dev = read_dataset(\"./RU/RU/dev.in\", False)\n",
    "ru_dev_label = labelling(ru_dev, ru_trainSet)\n",
    "write_dataset(\"./RU/RU/dev.p2.out\", ru_dev_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*insert some report stuff*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "Write a function that estimates the transition parameters from the training set using MLE (maximum\n",
    "likelihood estimation):\n",
    "\n",
    "`q(yi|yi−1) = Count(yi−1, yi) / Count(yi−1)`\n",
    "\n",
    "Please make sure the following special cases are also considered: q(STOP|yn) and q(y1|START)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_para(dataset, k=1):\n",
    "    states = defaultdict(lambda  : defaultdict(int))\n",
    "    y_count = defaultdict(int)\n",
    "    y_count['START'] = len(dataset)\n",
    "    y_count['STOP'] = len(dataset)\n",
    "    for sent in dataset:\n",
    "        n = len(sent)\n",
    "        for i in range(n):\n",
    "            y_current = sent[i][1]\n",
    "            y_count[y_current] += 1\n",
    "            if i == 0:\n",
    "                states[y_current]['START'] += 1\n",
    "            elif i == n-1:\n",
    "                states['STOP'][y_current] += 1\n",
    "                states[y_current][sent[i-1][1]] += 1\n",
    "            else:\n",
    "                states[y_current][sent[i-1][1]] += 1\n",
    "    def q(yi, yi_1):\n",
    "        if states[yi][yi_1] == 0:\n",
    "            return k/(y_count[yi_1] + k) # smoothing\n",
    "        return states[yi][yi_1]/((y_count)[yi_1] + k)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the estimated transition and emission parameters, implement the Viterbi algorithm to compute\n",
    "the following (for a sentence with n words):\n",
    "\n",
    "```\n",
    "y1,...,yn = arg max p(x1,...,xn, y1,...,yn)\n",
    "           y1,...,yn\n",
    "```\n",
    "\n",
    "For all datasets, learn the model parameters with `train`. Run the Viterbi algorithm on the development set `dev.in` using the learned models, write your output to `dev.p2.out` for the two datasets respectively. Report the precision, recall and F scores of all systems.\n",
    "\n",
    "*Note: in case you encounter potential numerical underflow issue, think of a way to address such an\n",
    "issue in your implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "Use the estimated transition and emission parameters, implement an algorithm to find the 5-th best\n",
    "output sequences (if there are multiple sequences that are 5-th best, output any of them). Clearly\n",
    "describe the steps of your algorithm in your report.\n",
    "\n",
    "Run the algorithm on the development sets `RU/dev.in` and `ES/dev.in`. Write the outputs to\n",
    "`RU/dev.p3.out` and `ES/dev.p3.out`. Report the precision, recall and F scores for the outputs\n",
    "for both languages.\n",
    "\n",
    "*Hint: find the top-5 best sequences using dynamic programming by modifying the original Viterbi\n",
    "algorithm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "Now, based on the training and development set, think of a better design for developing an improved sentiment analysis system for tweets using any model you like. Please explain clearly the model/method that you used for designing the new system. We will check your code and may call you for an interview if we have questions about your code. Please run your system on the development set `RU/dev.in` and `ES/dev.in`.\n",
    "\n",
    "Write your outputs to `RU/dev.p4.out` and `ES/dev.p4.out`.\n",
    "\n",
    "Report the precision, recall and F scores of your new systems for these two languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate your system’s performance on two held out test sets `RU/test.in` and `ES/test.in`. The test sets will only be released on 48 hours before the deadline. Use your new system to generate the outputs.\n",
    "\n",
    "Write your outputs to `RU/test.p4.out` and `ES/test.p4.out`.\n",
    "\n",
    "The system that achieves the overall highest F score on the test sets will be announced as the winner.\n",
    "\n",
    "*Hints: can we handle the new words in a better way? Are there better ways to model the transition and emission probabilities? Or can we use a discriminative approach instead of the generative approach? Perhaps using Perceptron? Any other creative ideas? Note that you are allowed to look into the scientific literature for ideas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aecd030d4c8316a52bf122072e28f84bcc79844c2684e041fef2e3f1d9f59078"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
